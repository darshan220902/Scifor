{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "-jfD8MxlPgXe",
        "outputId": "c7750d99-2700-4a5f-bff3-39633e5616a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ1. Describe the decision tree classifier algorithm and how it works to make predictions.\\nAns - A decision tree classifier is a supervised machine learning algorithm used for both classification and regression tasks\\n\\nQ2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\\nAns - Gini impurity : It is a measure of how often a randomly chosen element would be incorrectly classified\\n      Gini=1−summation(pi**2) where i==1 to k\\n      Gini=0: The set is pure\\n      Gini=1: The set is maximally impure\\n\\n      Entropy: Entropy is a measure of impurity, disorder, or randomness within a set of data\\n      H=−sumation(pi.log2(pi)) where i==1 to k\\n      H=0: The set is perfectly pure\\n      H=1: The set is perfectly impure\\n\\n      Information Gain :The information gain for a given feature is calculated by measuring \\n      the reduction in entropy before and after the split based on that feature\\n      Information Gain=Entropy(parent)-summation(subset/parent.entropy(subset(i)))\\n\\nQ3. Explain how a decision tree classifier can be used to solve a binary classification problem.\\nAns-  a decision tree classifier separates the dataset into two classes\\n                             |\\n                             |\\n                             V\\n      algorithm selects the best feature and split point to minimize impurity, such as Gini impurity or maximize information gain\\nQ4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\\npredictions.\\nAns-a decision tree classifier divides the feature space into rectangular regions with axis-parallel splits.\\n     Each split is based on a specific feature, creating a hierarchical structure. To predict a new instance, \\n     traverse the tree from root to leaf, assigning the majority class in the leaf node\\n\\n\\nQ5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\\nclassification model.\\nQ6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\\ncalculated from it.\\nQ7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\\nexplain how this can be done.\\nQ8. Provide an example of a classification problem where precision is the most important metric, and\\nexplain why.\\nQ9. Provide an example of a classification problem where recall is the most important metric, and explain\\nwhy.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
        "Ans - A decision tree classifier is a supervised machine learning algorithm used for both classification\n",
        "      and regression tasks\n",
        "\n",
        "                    Decision Tree Classifier Algorithm\n",
        "\n",
        "                            [Root Node]\n",
        "                            /         \\\n",
        "                  [Split Node]    [Split Node]\n",
        "                  /      |          /        \\\n",
        "          [Leaf Node] [Leaf Node] [Leaf Node] [Leaf Node]\n",
        "\n",
        "      Root Node: The tree starts with a root node containing the entire dataset.\n",
        "      Split Nodes:Selects the best feature to split the data into subsets.\n",
        "      Leaf Nodes: The process is recursively applied, creating a tree structure with\n",
        "      internal nodes representing splits and leaf nodes representing the predicted classes.\n",
        "      Prediction: To predict a new instance, traverse the tree from the root to a\n",
        "      leaf node based on the feature values\n",
        "\n",
        "\n",
        "\n",
        "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
        "Ans - Gini impurity : It is a measure of how often a randomly chosen element would be incorrectly classified\n",
        "      Gini=1−summation(pi**2) where i==1 to k\n",
        "      Gini=0: The set is pure\n",
        "      Gini=1: The set is maximally impure\n",
        "\n",
        "      Entropy: Entropy is a measure of impurity, disorder, or randomness within a set of data\n",
        "      H=−sumation(pi.log2(pi)) where i==1 to k\n",
        "      H=0: The set is perfectly pure\n",
        "      H=1: The set is perfectly impure\n",
        "\n",
        "      Information Gain :The information gain for a given feature is calculated by measuring\n",
        "      the reduction in entropy before and after the split based on that feature\n",
        "\n",
        "      Information Gain=Entropy(parent)-summation(subset/parent.entropy(subset(i)))\n",
        "\n",
        "\n",
        "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
        "\n",
        "Ans-  1) separates the dataset into two classes\n",
        "      2)algorithm selects the best feature and split point to minimize impurity\n",
        "      3)process is recursively applied\n",
        "      4)tree-building continues until a stopping criterion is met\n",
        "\n",
        "                Decision Tree for Binary Classification\n",
        "\n",
        "                            [Root Node]\n",
        "                             /         \\\n",
        "              [Split Node] (Feature A > 0.5)   (Feature B < 2.0)\n",
        "              /      |                                   /        \\\n",
        "     [Leaf Node] [Leaf Node]                         [Leaf Node] [Leaf Node]\n",
        "   (Class 0)   (Class 1)                            (Class 1)   (Class 0)\n",
        "\n",
        "\n",
        "\n",
        "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
        "predictions.\n",
        "Ans-a decision tree classifier divides the feature space into rectangular regions with axis-parallel splits.\n",
        "     Each split is based on a specific feature, creating a hierarchical structure. To predict a new instance,\n",
        "     traverse the tree from root to leaf, assigning the majority class in the leaf node\n",
        "\n",
        "\n",
        "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
        "classification model.\n",
        "Ans - It compares the predicted and true class labels for a set of instances\n",
        "      True Positive (TP): Instances correctly predicted as positive.\n",
        "      True Negative (TN): Instances correctly predicted as negative.\n",
        "      False Positive (FP): Instances incorrectly predicted as positive .\n",
        "      False Negative (FN): Instances incorrectly predicted as negative.\n",
        "\n",
        "\n",
        "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
        "calculated from it.\n",
        "Ans-                  Predicted\n",
        "                Positive    Negative\n",
        "Actual  Positive     58        8\n",
        "        Negative     9        135\n",
        "\n",
        "        Precision: Precision is the ratio of correctly predicted positive instances to the total predicted positives.\n",
        "        Precision = TP / (TP + FP) = 58 / (58 + 9) = 0.865.\n",
        "\n",
        "        Recall: Recall is the ratio of correctly predicted positive instances to the total actual positives.\n",
        "        Recall = TP / (TP + FN) = 58 / (58 + 8) = 0.878.\n",
        "\n",
        "        F1 Score: F1 score is the harmonic mean of precision and recall.\n",
        "        F1 Score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.865 * 0.878) / (0.865 + 0.878) = 0.871.\n",
        "\n",
        "\n",
        "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
        "explain how this can be done.\n",
        "Ans- Accuracy is a common metric but may not be suitable for imbalanced datasets.\n",
        "     Precision is important when false positives are costly.\n",
        "     Recall is crucial when false negatives are costly.\n",
        "     F1 score balances precision and recall.\n",
        "\n",
        "     To choose the right metric:\n",
        "\n",
        "      Understand the Problem: Consider the consequences of false positives and\n",
        "      false negatives in the context of the problem.\n",
        "\n",
        "      Imbalance: If the classes are imbalanced, accuracy might not be a reliable metric.\n",
        "      Consider precision, recall, or F1 score.\n",
        "\n",
        "      Business Objectives: Align the choice of metric with the business objectives.\n",
        "      For instance, in fraud detection, recall might be more critical.\n",
        "\n",
        "\n",
        "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
        "explain why.\n",
        "Ans- Spam email detection system helps us to clasify the mail is spam or ham\n",
        "     Precision focuses on the accuracy of positive predictions, making it a more relevant\n",
        "     metric in scenarios where the cost of false positives is high.\n",
        "\n",
        "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
        "why.\n",
        "ans- a fraud detection system for financial transactions. In this case, the objective is to catch as many fraudulent transactions as possible.\n",
        "    Missing a fraudulent activity (false negative) could result in financial losses for both the users and the financial institution.\n",
        "   Recall is a vital metric in this context because it emphasizes minimizing false negatives and capturing as many instances of fraud as possible\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-Q3heaXT29A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FppgyYtJTmTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}